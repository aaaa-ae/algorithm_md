## **1、GoR训练阶段完整流程**

GoR 的核心思想是：利用 **模拟查询 + LLM 生成的历史响应**，把长文档组织成一个 **图结构**，然后用 **GNN 学习节点嵌入**，通过 **自监督目标（对比学习 + 排序约束）** 训练得到更优的检索器。

---

### 阶段 1：数据准备 & 查询模拟

1. **文档切分**

   - 将长文档拆成一系列小块：

     C={c1,c2,…,cm}C = \{c_1, c_2, \dots, c_m\}

   每块大小 256 tokens，重叠 32 tokens（论文里用 LangChain 的 TokenTextSplitter）。

2. **生成模拟查询**

   - 从文档中随机采样一个 chunk csc_s。
   - 用 LLM（例如 Mixtral-8x7B-Instruct）生成一个基于 csc_s 的查询 qsq_s。
   - 重复直到生成大约 **30 个不重复的查询**（论文发现 30 个就足够，更多也许更好但受限于计算预算）。

3. **构造伪训练集**
    得到：

   T={(qsi,csi)}i=1∣T∣T = \{(q_s^i, c_s^i)\}_{i=1}^{|T|}

   作为自监督训练数据。

---

### 阶段 2：图构建（Graph Construction）

1. **RAG 流程**
   - 给定一个模拟查询 qsiq_s^i，用基础检索器（Contriever）从 CC（以及之前的历史响应）里检索出 K 个 chunk。
   - 将这些 chunk 和查询输入 LLM，生成响应 rir_i。
2. **图的节点和边**
   - 节点包括：
     - 原始 chunk 节点 cc
     - LLM 历史响应节点 rr
   - 每次 RAG 会把检索到的 K 个 chunk 节点与新生成的响应节点 rir_i 建立边。
   - 同时，把 rir_i 加入检索库，以便后续查询也能检索到它。

👉 最终得到一个“图”，其中响应节点是 chunk 节点之间的“桥梁”，把分散的 chunk 关联起来。

---

### 阶段 3：GNN 表示学习

1. **初始化**

   - 使用 Contriever 的上下文编码器 Ec(⋅)E_c(\cdot) 初始化节点向量。

2. **图神经网络传播**

   - 使用 **两层 Graph Attention Network (GAT)**（隐藏维度 768）进行迭代更新：

     h_v^{(l)} = \text{MLP}\big(h_v^{(l-1)} + \text{POOL}(\{h_u^{(l-1)} | u \in N(v)\})\big) \]:contentReference[oaicite:3]{index=3}  

   - 学到的 embedding 融合了节点本身和邻居节点的语义。

---

### 阶段 4：自监督目标设计

#### 4.1 主要问题

- 没有“标注好的相关 chunk”作为训练标签。
- 全局参考摘要也不能直接对齐节点。

#### 4.2 解决方法：BERTScore 排序

- 计算每个节点文本和 **参考摘要** 的 BERTScore 相似度。
- 得到一个 **节点排序表 MiM_i**，作为间接监督信号。

#### 4.3 对比学习 (Contrastive Learning)

- 对于每个模拟查询 qsiq_s^i，其对应的 chunk csic_s^i 被视为正样本（positive），其他节点作为负样本。

- 使用 InfoNCE 损失：

  LCL=−1∣T∣∑jlog⁡s(qj,h+)s(qj,h+)+∑is(qj,hi)L_{CL} = - \frac{1}{|T|} \sum_j \log \frac{s(q_j, h_+)}{s(q_j, h_+) + \sum_i s(q_j, h_i)}

  其中 s(q,h)=exp⁡(Eq(q)⊤h/τ)s(q, h) = \exp(E_q(q)^\top h / \tau)。

#### 4.4 排序损失 (Pair-wise Ranking Loss)

- 在节点排名列表 MiM_i 上施加约束：
   如果节点 hjh_j 应该比 hih_i 排名更高，则优化方向为：

  Eq(q)⊤hj<Eq(q)⊤hiE_q(q)^\top h_j < E_q(q)^\top h_i

- 损失函数：

  LRANK=∑(i,j)log⁡(1+s(q,hj)s(q,hi))L_{RANK} = \sum_{(i,j)} \log \Big(1 + \frac{s(q,h_j)}{s(q,h_i)}\Big)

#### 4.5 总目标

L=LCL+αLRANKL = L_{CL} + \alpha L_{RANK}

---

### 阶段 5：训练细节

- **训练单位**：每个长文档对应一张 GoR 图。
- **Batch 内负样本**：不仅用同一图里的负样本，还用 batch 内其他图的节点作为负样本（完全不相关，更有对比性）。
- **效率**：只有 GNN 是可训练模块，LLM 和 Contriever 都冻结，因此训练成本很低。

---

---

## **2、GoR 推理阶段完整流程**

训练完成后，GoR 不再是一个需要“再学习”的模型，而是充当一个 **高级检索器**。它和标准 RAG 的使用流程完全一致，只是在检索环节更强大，因为它利用了 GNN 融合了文本片段与 LLM 历史响应的关联信息。

---

### **第一步：准备阶段（Graph Construction & Embedding）**

在使用 GoR 做推理之前，需要先为目标长文档构建好一个图，并为其中的节点生成嵌入：

1. **文档切块**
   - 将长文档拆分为一系列固定长度的文本片段 C={c1,c2,…,cm}C = \{c₁, c₂, …, cₘ\}。
2. **查询模拟与历史响应生成**
   - 用 LLM 生成若干模拟查询，并运行 RAG 流程（检索 → 生成响应）。
   - 得到的响应 rjrⱼ 作为新节点加入图，并与当时检索到的 Top-K 片段建立边。
3. **图神经网络编码**
   - 在构建好的“记录之图（GoR）”上运行 GNN（两层 GAT），
   - 为所有节点（文档片段 cᵢ 和历史响应 rⱼ）计算出优化后的节点嵌入。

👉 此时，这个 **GoR 图及其节点向量** 就是你的知识库，在推理阶段会被直接加载使用。

---

### **第二步：检索阶段（Query → Node Embeddings）**

当用户输入查询 Q 时（例如“请总结这篇论文”）：

1. **编码查询**
   - 使用查询编码器 Eq(⋅)E_q(·) 将 Q 编码为查询向量 Eq(Q)E_q(Q)。
2. **相似度计算**
   - 计算 Eq(Q)E_q(Q) 与图中所有节点嵌入之间的相似度。
3. **Top-K 选择**
   - 按照相似度得分排序，选出最相关的 Top-K 节点。
   - 这些节点既可能是 **原始文档片段 cᵢ**，也可能是 **LLM 历史响应 rⱼ**。

👉 与标准 RAG 不同，GoR 的节点嵌入更“全局”，因为它们已经整合了图中的上下文关系。

---

### **第三步：生成阶段（Node Text → LLM）**

1. **取回原始文本**

   - 检索阶段输出的节点最终都对应真实的文本（chunk 或响应），这里取出它们的原始内容。

2. **拼接提示（Prompt Construction）**

   - 将这些文本按一定顺序拼接在一起，并加上用户查询，形成 LLM 的输入：

     ```
     请参考以下背景材料，回答用户问题：
     
     [背景材料开始]
     {节点1的文本内容}
     {节点2的文本内容}
     ...
     {节点K的文本内容}
     [背景材料结束]
     
     用户问题：{Q}
     ```

3. **Tokenization & Embedding**

   - 拼接后的文本 → tokenizer → token IDs → token embeddings。

4. **输入 LLM 生成**

   - 将 token embeddings 输入 LLM（如 LLaMA-2-7B-chat），通过 Transformer 层层计算，逐步预测下一个 token，最终生成一个完整的摘要。

👉 与标准 RAG 一致，但因为 GoR 检索到的上下文更优，最终的摘要质量更高。

---

---

## **3、RAG vs GoR 推理阶段对比**

| 步骤           | 标准 RAG                                | GoR                                          |
| -------------- | --------------------------------------- | -------------------------------------------- |
| **文档预处理** | 文档切块 → 向量化                       | 文档切块 + 历史响应 → 构建图 → GNN 节点嵌入  |
| **检索阶段**   | 用查询向量在 chunk 向量库里做相似度检索 | 用查询向量在 **图节点嵌入库** 里做相似度检索 |
| **Top-K 内容** | 只返回原始文档片段                      | 返回文档片段 + 可能的历史响应（更浓缩）      |
| **生成阶段**   | Top-K 片段拼接 → LLM                    | Top-K 节点文本拼接（信息更全局）→ LLM        |
| **结果质量**   | 局部相关性好，但可能丢失全局关键信息    | 全局相关性更强，摘要更全面、浓缩度更高       |

------

✅ **一句话总结**：
 推理时，GoR 就是一个“经过图优化的检索器”。流程和标准 RAG 一样：**检索 → 拼接 → 喂给 LLM**，但因为检索节点融合了片段和历史响应的全局信息，所以生成的摘要更优质。

---

---

## 4、问题模拟流程

## 🔹 整个流程

（把长文本分割成多个片段，然后随机选择一个片段交给LLM，来生成一个问题Q（采用temperature sampling），然后再随机选择一个片段并生成Q，反复这个操作；在过程中可能随机选到同一个片段，所以此时temperature sampling就产生作用了，能够保证答案不一样，如果是贪婪解码模式，对于同一个片段，答案是一样的）

1. 把长文档切成很多小片段 C={c1,c2,…,cn}C = \{c_1, c_2, \dots, c_n\}C={c1,c2,…,cn}。
2. **随机选一个片段 csc_scs**。
3. 把 csc_scs 丢给 LLM，让它生成一个查询 qsq_sqs。
   - 如果用 **贪婪解码 (greedy decoding)**：
     - 每次都会生成几乎相同的问题（因为模型总是选概率最高的词）。
     - → 缺乏多样性。
   - 如果用 **温度采样 (temperature sampling)**：
     - 在生成过程中引入随机性，即使是同一个片段，也可能得到不同的问题。
     - → 提高了问题的多样性和覆盖度。
4. 重复步骤 2–3，直到得到足够多的 **非重复问题集**。

---

---

## **5、GraphRAG vs GoR**

## 🔹1. GraphRAG 的“全局”

- **全局主题结构**：通过大规模图索引 + 社区检测，找到一个“主题簇”（比如新闻事件的完整脉络、一本书的章节结构）。
- 偏向 **外部结构上的全局性**。
   👉 举例：如果你要总结“俄乌战争新闻全貌”，GraphRAG 会帮你找到所有和这个主题相关的新闻片段，保证主题覆盖完整。

------

## 🔹2. GoR 的“全局”

- **全局 summarization 能力**：
  - GoR 的任务场景是 **长文档摘要**，需要覆盖整篇文档的关键信息（不是只回答某个局部问题）。
  - 它通过 **GNN 建模片段之间 & 片段与 LLM 历史响应之间的关系**，让模型更好地区分哪些片段对“全局总结”重要。
- 偏向 **内容提炼上的全局性**。
   👉 举例：如果你要总结“一篇 50 页学术论文”，GoR 并不会像 GraphRAG 那样做主题社区划分，而是学会在所有片段中找到“能代表整篇文章”的关键内容（比如方法、实验结果），然后生成摘要。

------

## 🔹3. 两种“全局”的区别

| 方法         | “全局”指什么                                       | 典型任务                        |
| ------------ | -------------------------------------------------- | ------------------------------- |
| **GraphRAG** | 全局主题覆盖（把同一主题下的片段聚类）             | 多文档检索、事件追踪、知识库 QA |
| **GoR**      | 全局信息提炼（从所有片段里学到整体总结的关键内容） | 长文档摘要、综合报告生成        |

------

## 🔹4. GoR 的全局能力总结

- 它不是通过 **外部图结构** 来保证“全局主题覆盖”；
- 而是通过 **GNN 节点嵌入学习**，在检索片段 + 历史响应之间建立联系，
- 让 LLM 在生成时更容易聚焦于“能代表全局的重要信息”，而不是停留在局部相关性上。

------

✅ **一句话总结**：
 GoR 的“全局能力”主要体现在 **帮助 LLM 在长文档中做全面摘要**，保证覆盖面和代表性；而 GraphRAG 的“全局能力”是 **通过图社区来覆盖整个主题**，两者的“全局”侧重点不同。

---

---

## 6、训练 vs 推理

| 环节         | 训练阶段                                    | 推理阶段                             |
| ------------ | ------------------------------------------- | ------------------------------------ |
| **模拟查询** | 必须，有监督信号的来源                      | 仍然需要，用来覆盖文档，但不产生梯度 |
| **图的构建** | 每个模拟查询都触发一次检索+生成，图逐步扩展 | 构建一次完整的图，帮助检索           |
| **GNN**      | 学习节点 embedding，更新参数                | 使用训练好的参数，生成节点 embedding |
| **目标**     | 优化损失函数 (CL + RANK)                    | 提供高质量片段给 LLM                 |
| **输出**     | 优化过的模型参数                            | LLM 生成的最终摘要                   |